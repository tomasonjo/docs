---
title: Model Context Protocol (MCP)
---

:::python
[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) library.
:::
:::js
[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters/adapters) library.
:::

## Install

:::python
Install the `langchain-mcp-adapters` library to use MCP tools in LangGraph:

<CodeGroup>
```bash pip
pip install langchain-mcp-adapters
```

```bash uv
uv add langchain-mcp-adapters
```
</CodeGroup>
:::

:::js
Install the `@langchain/mcp-adapters` library to use MCP tools in LangGraph:

<CodeGroup>
```bash npm
npm install @langchain/mcp-adapters
```

```bash pnpm
pnpm add @langchain/mcp-adapters
```

```bash yarn
yarn add @langchain/mcp-adapters
```

```bash bun
bun add @langchain/mcp-adapters
```
</CodeGroup>
:::

## Transport types

MCP supports different transport mechanisms for client-server communication:

- **stdio** – Client launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.
- **Streamable HTTP** – Server runs as an independent process handling HTTP requests. Supports remote connections and multiple clients.
- **Server-Sent Events (SSE), Legacy ** – a variant of streamable HTTP optimized for real-time streaming communication.

## Use MCP tools

:::python
`langchain-mcp-adapters` enables agents to use tools defined across one or more MCP server.

```python Accessing multiple MCP servers icon="server"
from langchain_mcp_adapters.client import MultiServerMCPClient  # [!code highlight]
from langchain.agents import create_agent


client = MultiServerMCPClient(  # [!code highlight]
    {
        "math": {
            "transport": "stdio",  # Local subprocess communication
            "command": "python",
            # Absolute path to your math_server.py file
            "args": ["/path/to/math_server.py"],
        },
        "weather": {
            "transport": "streamable_http",  # HTTP-based remote server
            # Ensure you start your weather server on port 8000
            "url": "http://localhost:8000/mcp",
        }
    }
)

tools = await client.get_tools()  # [!code highlight]
agent = create_agent(
    "claude-sonnet-4-5-20250929",
    tools  # [!code highlight]
)
math_response = await agent.ainvoke(
    {"messages": [{"role": "user", "content": "what's (3 + 5) x 12?"}]}
)
weather_response = await agent.ainvoke(
    {"messages": [{"role": "user", "content": "what is the weather in nyc?"}]}
)
```
:::

:::js
`@langchain/mcp-adapters` enables agents to use tools defined across one or more MCP server.

```ts Accessing multiple MCP servers icon="server"
import { MultiServerMCPClient } from "@langchain/mcp-adapters";  // [!code highlight]
import { ChatAnthropic } from "@langchain/anthropic";
import { createAgent } from "langchain";

const client = new MultiServerMCPClient({  // [!code highlight]
    math: {
        transport: "stdio",  // Local subprocess communication
        command: "node",
        // Replace with absolute path to your math_server.js file
        args: ["/path/to/math_server.js"],
    },
    weather: {
        transport: "sse",  // Server-Sent Events for streaming
        // Ensure you start your weather server on port 8000
        url: "http://localhost:8000/mcp",
    },
});

const tools = await client.getTools();  // [!code highlight]
const agent = createAgent({
    model: "claude-sonnet-4-5-20250929",
    tools,  // [!code highlight]
});

const mathResponse = await agent.invoke({
    messages: [{ role: "user", content: "what's (3 + 5) x 12?" }],
});

const weatherResponse = await agent.invoke({
    messages: [{ role: "user", content: "what is the weather in nyc?" }],
});
```
:::

<Note>
    `MultiServerMCPClient` is **stateless by default**. Each tool invocation creates a fresh MCP `ClientSession`, executes the tool, and then cleans up.
</Note>

## Custom MCP servers

:::python

To create a custom MCP server, you can use the [FastMCP](https://gofastmcp.com/getting-started/welcome) library.


<CodeGroup>
```bash pip
pip install fastmcp
```

```bash uv
uv add fastmcp
```
</CodeGroup>
:::

:::js
To create your own MCP servers, you can use the `@modelcontextprotocol/sdk` library. This library provides a simple way to define [tools](https://modelcontextprotocol.io/docs/learn/server-concepts#tools-ai-actions) and run them as servers.

<CodeGroup>
```bash npm
npm install @modelcontextprotocol/sdk
```

```bash pnpm
pnpm add @modelcontextprotocol/sdk
```

```bash yarn
yarn add @modelcontextprotocol/sdk
```

```bash bun
bun add @modelcontextprotocol/sdk
```
</CodeGroup>
:::

Use the following reference implementations to test your agent with MCP tool servers.


:::python
<CodeGroup>
```python title="Math server (stdio transport)" icon="floppy-disk"
from fastmcp import FastMCP

mcp = FastMCP("Math")

@mcp.tool()
def add(a: int, b: int) -> int:
    """Add two numbers"""
    return a + b

@mcp.tool()
def multiply(a: int, b: int) -> int:
    """Multiply two numbers"""
    return a * b

if __name__ == "__main__":
    mcp.run(transport="stdio")
```

```python title="Weather server (streamable HTTP transport)" icon="wifi"
from fastmcp import FastMCP

mcp = FastMCP("Weather")

@mcp.tool()
async def get_weather(location: str) -> str:
    """Get weather for location."""
    return "It's always sunny in New York"

if __name__ == "__main__":
    mcp.run(transport="streamable-http")
```
</CodeGroup>
:::

:::js
```typescript title="Math server (stdio transport)" icon="floppy-disk"
import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import {
    CallToolRequestSchema,
    ListToolsRequestSchema,
} from "@modelcontextprotocol/sdk/types.js";

const server = new Server(
    {
        name: "math-server",
        version: "0.1.0",
    },
    {
        capabilities: {
            tools: {},
        },
    }
);

server.setRequestHandler(ListToolsRequestSchema, async () => {
    return {
        tools: [
        {
            name: "add",
            description: "Add two numbers",
            inputSchema: {
                type: "object",
                properties: {
                    a: {
                        type: "number",
                        description: "First number",
                    },
                    b: {
                        type: "number",
                        description: "Second number",
                    },
                },
                required: ["a", "b"],
            },
        },
        {
            name: "multiply",
            description: "Multiply two numbers",
            inputSchema: {
                type: "object",
                properties: {
                    a: {
                        type: "number",
                        description: "First number",
                    },
                    b: {
                        type: "number",
                        description: "Second number",
                    },
                },
                required: ["a", "b"],
            },
        },
        ],
    };
});

server.setRequestHandler(CallToolRequestSchema, async (request) => {
    switch (request.params.name) {
        case "add": {
            const { a, b } = request.params.arguments as { a: number; b: number };
            return {
                content: [
                {
                    type: "text",
                    text: String(a + b),
                },
                ],
            };
        }
        case "multiply": {
            const { a, b } = request.params.arguments as { a: number; b: number };
            return {
                content: [
                {
                    type: "text",
                    text: String(a * b),
                },
                ],
            };
        }
        default:
            throw new Error(`Unknown tool: ${request.params.name}`);
    }
});

async function main() {
    const transport = new StdioServerTransport();
    await server.connect(transport);
    console.error("Math MCP server running on stdio");
}

main();
```

```typescript title="Weather server (SSE transport)" icon="wifi"
import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { SSEServerTransport } from "@modelcontextprotocol/sdk/server/sse.js";
import {
    CallToolRequestSchema,
    ListToolsRequestSchema,
} from "@modelcontextprotocol/sdk/types.js";
import express from "express";

const app = express();
app.use(express.json());

const server = new Server(
    {
        name: "weather-server",
        version: "0.1.0",
    },
    {
        capabilities: {
            tools: {},
        },
    }
);

server.setRequestHandler(ListToolsRequestSchema, async () => {
    return {
        tools: [
        {
            name: "get_weather",
            description: "Get weather for location",
            inputSchema: {
            type: "object",
            properties: {
                location: {
                type: "string",
                description: "Location to get weather for",
                },
            },
            required: ["location"],
            },
        },
        ],
    };
});

server.setRequestHandler(CallToolRequestSchema, async (request) => {
    switch (request.params.name) {
        case "get_weather": {
            const { location } = request.params.arguments as { location: string };
            return {
                content: [
                    {
                        type: "text",
                        text: `It's always sunny in ${location}`,
                    },
                ],
            };
        }
        default:
            throw new Error(`Unknown tool: ${request.params.name}`);
    }
});

app.post("/mcp", async (req, res) => {
    const transport = new SSEServerTransport("/mcp", res);
    await server.connect(transport);
});

const PORT = process.env.PORT || 8000;
app.listen(PORT, () => {
    console.log(`Weather MCP server running on port ${PORT}`);
});
```
:::

## Passing headers

:::python
When connecting to MCP servers over HTTP, you can include custom headers (e.g., for authentication or tracing) using the `headers` field in the connection configuration. This is supported for `sse` and `streamable_http` transports.

```python Passing headers with MultiServerMCPClient
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain.agents import create_agent

client = MultiServerMCPClient(
    {
        "weather": {
            "transport": "streamable_http",
            "url": "http://localhost:8000/mcp",
            "headers": {  # [!code highlight]
                "Authorization": "Bearer YOUR_TOKEN",  # [!code highlight]
                "X-Custom-Header": "custom-value"  # [!code highlight]
            },  # [!code highlight]
        }
    }
)
tools = await client.get_tools()
agent = create_agent("openai:gpt-4.1", tools)
response = await agent.ainvoke({"messages": "what is the weather in nyc?"})
```
:::

## Interceptors

:::python
Interceptors wrap tool execution to enable request/response modification, retry logic, caching, rate limiting, and other cross-cutting concerns. They follow an "onion" pattern where the first interceptor in the list is the outermost layer.

### Basic interceptor

An interceptor is an async function that receives a request and a handler. You can modify the request before calling the handler, modify the response after, or skip the handler entirely.

```python Basic interceptor pattern
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.interceptors import MCPToolCallRequest

async def logging_interceptor(
    request: MCPToolCallRequest,
    handler,
):
    """Log tool calls before and after execution."""
    print(f"Calling tool: {request.name} with args: {request.args}")
    result = await handler(request)
    print(f"Tool {request.name} returned: {result}")
    return result

client = MultiServerMCPClient(
    {"math": {"transport": "stdio", "command": "python", "args": ["/path/to/server.py"]}},
    tool_interceptors=[logging_interceptor],  # [!code highlight]
)
```

### Modifying requests

Use `request.override()` to create a modified request. This follows an immutable pattern, leaving the original request unchanged.

```python Modifying tool arguments
async def double_args_interceptor(
    request: MCPToolCallRequest,
    handler,
):
    """Double all numeric arguments before execution."""
    modified_args = {k: v * 2 for k, v in request.args.items()}
    modified_request = request.override(args=modified_args)  # [!code highlight]
    return await handler(modified_request)

# Original call: add(a=2, b=3) becomes add(a=4, b=6)
```

### Modifying headers at runtime

Interceptors can modify HTTP headers dynamically based on the request context:

```python Dynamic header modification
async def auth_header_interceptor(
    request: MCPToolCallRequest,
    handler,
):
    """Add authentication headers based on the tool being called."""
    token = get_token_for_tool(request.name)
    modified_request = request.override(
        headers={"Authorization": f"Bearer {token}"}  # [!code highlight]
    )
    return await handler(modified_request)
```

### Caching

```python Caching interceptor
cache = {}

async def caching_interceptor(
    request: MCPToolCallRequest,
    handler,
):
    """Cache tool results to avoid repeated calls."""
    cache_key = f"{request.name}:{request.args}"

    if cache_key in cache:
        return cache[cache_key]  # Return cached result

    result = await handler(request)
    cache[cache_key] = result
    return result
```

### Short-circuiting execution

Interceptors can skip the handler entirely and return a custom result:

```python Short-circuit interceptor
from mcp.types import CallToolResult, TextContent

async def mock_interceptor(
    request: MCPToolCallRequest,
    handler,
):
    """Return mock data for testing."""
    if request.name == "get_weather":
        return CallToolResult(  # [!code highlight]
            content=[TextContent(type="text", text="Mocked: Sunny, 72°F")],  # [!code highlight]
            isError=False,  # [!code highlight]
        )  # [!code highlight]
    return await handler(request)
```

### Composing interceptors

Multiple interceptors compose in "onion" order — the first interceptor in the list is the outermost layer:

```python Composing multiple interceptors
async def outer_interceptor(request, handler):
    print("outer: before")
    result = await handler(request)
    print("outer: after")
    return result

async def inner_interceptor(request, handler):
    print("inner: before")
    result = await handler(request)
    print("inner: after")
    return result

client = MultiServerMCPClient(
    {...},
    tool_interceptors=[outer_interceptor, inner_interceptor],  # [!code highlight]
)

# Execution order:
# outer: before -> inner: before -> tool execution -> inner: after -> outer: after
```

### Accessing LangGraph runtime

When MCP tools are used within a LangGraph agent (via `create_agent`), the interceptor receives access to the `ToolRuntime` context. This provides access to the tool call ID, state, config, and store.

```python Accessing ToolRuntime in interceptors
from langchain.tools import ToolRuntime
from langchain_core.messages import ToolMessage
from langgraph.types import Command

async def runtime_interceptor(
    request: MCPToolCallRequest,
    handler,
):
    """Access LangGraph runtime context."""
    runtime: ToolRuntime = request.runtime  # [!code highlight]

    # Access the tool call ID
    tool_call_id = runtime.tool_call_id  # [!code highlight]

    # Access agent state (if using InjectedState)
    # state = runtime.state

    # Access config (if using InjectedConfig)
    # config = runtime.config

    # Access store (if using InjectedStore)
    # store = runtime.store

    return await handler(request)
```

### Returning LangGraph Commands

Interceptors can return `Command` objects to update agent state or control graph execution:

```python Returning Command from interceptor
from langchain.agents import AgentState, create_agent
from langchain.tools import ToolRuntime
from langchain_core.messages import ToolMessage
from langgraph.types import Command

async def command_interceptor(
    request: MCPToolCallRequest,
    handler,
):
    """Return a Command to update agent state."""
    runtime: ToolRuntime = request.runtime

    # Instead of calling the tool, return a Command that updates state
    return Command(  # [!code highlight]
        update={  # [!code highlight]
            "counter": 42,  # [!code highlight]
            "messages": [  # [!code highlight]
                ToolMessage(  # [!code highlight]
                    content="Counter updated!",  # [!code highlight]
                    tool_call_id=runtime.tool_call_id,  # [!code highlight]
                ),  # [!code highlight]
            ],  # [!code highlight]
        },  # [!code highlight]
        goto="__end__",  # Optional: control graph flow  # [!code highlight]
    )  # [!code highlight]

# Use with custom agent state
class CustomAgentState(AgentState):
    counter: int = 0

agent = create_agent(
    model,
    tools,
    state_schema=CustomAgentState,
)
```
:::

## Callbacks

:::python
The MCP protocol supports [logging](https://modelcontextprotocol.io/specification/2025-03-26/server/utilities/logging#log-levels) and [progress notifications](https://modelcontextprotocol.io/specification/2025-03-26/basic/utilities/progress) from servers. Use the `Callbacks` class to subscribe to these events.

### Logging messages

Subscribe to log messages sent by MCP servers:

```python Logging callback
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.callbacks import Callbacks, CallbackContext
from mcp.types import LoggingMessageNotificationParams

async def on_logging_message(
    params: LoggingMessageNotificationParams,
    context: CallbackContext,
):
    """Handle log messages from MCP servers."""
    print(f"[{context.server_name}] {params.level}: {params.data}")

client = MultiServerMCPClient(
    {...},
    callbacks=Callbacks(on_logging_message=on_logging_message),  # [!code highlight]
)
```

### Progress notifications

Subscribe to progress updates for long-running tool executions:

```python Progress callback
from langchain_mcp_adapters.callbacks import Callbacks, CallbackContext

async def on_progress(
    progress: float,
    total: float | None,
    message: str | None,
    context: CallbackContext,
):
    """Handle progress updates from MCP servers."""
    percent = (progress / total * 100) if total else progress
    tool_info = f" ({context.tool_name})" if context.tool_name else ""
    print(f"[{context.server_name}{tool_info}] Progress: {percent:.1f}% - {message}")

client = MultiServerMCPClient(
    {...},
    callbacks=Callbacks(on_progress=on_progress),  # [!code highlight]
)
```

### Combined callbacks

```python Using both callbacks
from langchain_mcp_adapters.callbacks import Callbacks

callbacks = Callbacks(
    on_logging_message=on_logging_message,
    on_progress=on_progress,
)

client = MultiServerMCPClient(
    {
        "task_server": {
            "url": "http://localhost:8000/mcp",
            "transport": "streamable_http",
        }
    },
    callbacks=callbacks,
)
```

The `CallbackContext` provides:
- `server_name`: Name of the MCP server
- `tool_name`: Name of the tool being executed (available during tool calls)
:::

## Authentication

:::python
The `langchain-mcp-adapters` library uses the official [MCP SDK](https://github.com/modelcontextprotocol/python-sdk) under the hood, which allows you to provide a custom authentication mechanism by implementing the `httpx.Auth` interface.

```python
from langchain_mcp_adapters.client import MultiServerMCPClient

client = MultiServerMCPClient(
    {
        "weather": {
            "transport": "streamable_http",
            "url": "http://localhost:8000/mcp",
            "auth": auth, # [!code highlight]
        }
    }
)
```


* [Example custom auth implementation](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/clients/simple-auth-client/mcp_simple_auth_client/main.py)
* [Built-in OAuth flow](https://github.com/modelcontextprotocol/python-sdk/blob/main/src/mcp/client/auth.py#L179)
:::

## Stateful servers

If you need to control the lifecycle of an MCP session (for example, when working with a stateful server that maintains context across tool calls), you can create a persistent `ClientSession` using `client.session()`.

This gives you explicit control over the [session lifecycle](https://modelcontextprotocol.io/specification/2025-03-26/basic/lifecycle) — initializing, using, and then closing the session.

:::python
```python Using MCP ClientSession for stateful tool usage
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.tools import load_mcp_tools
from langchain.agents import create_agent

client = MultiServerMCPClient({...})

# Create a session explicitly
async with client.session("server_name") as session: # [!code highlight]
    # Pass the session to load tools
    tools = await load_mcp_tools(session) # [!code highlight]
    agent = create_agent(
        "anthropic:claude-3-7-sonnet-latest",
        tools
    )
```
:::

:::js
```typescript Using MCP ClientSession for stateful tool usage
import { loadMCPTools } from "@langchain/mcp-adapters/tools.js";
import { createAgent } from "langchain";

const client = new MultiServerMCPClient({...});
const session = await client.session("some_server");
const tools = await loadMCPTools(session);
const agent = createAgent({
    model: new ChatAnthropic({ model: "claude-3-7-sonnet-latest" }),
    tools,
});
```
:::

<Important>
    If you are serving your agent through a web server and intend to use it a conversational setting, be careful with how you manage sessions. A naive setup will create a **new session for each request**, which prevents state from being preserved across turns.
</Important>

## Structured content (artifacts)

:::python
MCP tools can return [structured content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#structured-content) alongside the human-readable text response. This is useful when a tool needs to return machine-parseable data (like JSON) in addition to text that gets shown to the model.

When an MCP tool returns `structuredContent`, the adapter wraps it in an `MCPToolArtifact` and returns it as the tool's artifact. You can access this using the `artifact` field on the `ToolMessage`:

```python Accessing structured content from tool results
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain.agents import create_agent

client = MultiServerMCPClient(
    {
        "data_server": {
            "transport": "streamable_http",
            "url": "http://localhost:8000/mcp",
        }
    }
)

tools = await client.get_tools()
agent = create_agent("anthropic:claude-sonnet-4-20250514", tools)

response = await agent.ainvoke(
    {"messages": [{"role": "user", "content": "Analyze this data"}]}
)

# Access tool messages from the response
for message in response["messages"]:
    if hasattr(message, "artifact") and message.artifact is not None:
        # The artifact contains the structured content from the MCP tool
        structured_data = message.artifact["structured_content"]  # [!code highlight]
        print(f"Structured content: {structured_data}")
```

### Creating an MCP server with structured content

Use [FastMCP](https://gofastmcp.com) to create servers that return structured content:

```python MCP server returning structured content
from fastmcp import FastMCP

mcp = FastMCP("DataAnalyzer")

@mcp.tool()
def analyze_data(dataset_id: str) -> tuple[str, dict]:
    """Analyze a dataset and return results.

    Returns both a human-readable summary and structured data.
    """
    # Perform analysis...
    results = {
        "dataset_id": dataset_id,
        "row_count": 1000,
        "columns": ["id", "name", "value"],
        "statistics": {
            "mean": 42.5,
            "median": 40.0,
            "std_dev": 12.3
        }
    }

    # Return (text_content, structured_content)
    return (  # [!code highlight]
        f"Analysis complete for dataset {dataset_id}: 1000 rows, 3 columns",  # [!code highlight]
        results  # This becomes structuredContent  # [!code highlight]
    )  # [!code highlight]

if __name__ == "__main__":
    mcp.run(transport="streamable-http")
```

### Artifact structure

The artifact returned by MCP tools has the following structure:

```python
from langchain_mcp_adapters.tools import MCPToolArtifact

# MCPToolArtifact is a TypedDict with:
# {
#     "structured_content": dict[str, Any]  # The structuredContent from the MCP tool
# }
```

This allows you to access rich, typed data from MCP tools while the model sees only the human-readable text content.
:::

## Additional resources

* [MCP documentation](https://modelcontextprotocol.io/introduction)
* [MCP Transport documentation](https://modelcontextprotocol.io/docs/concepts/transports)
:::python
* [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters)
:::
:::js
* [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters/)
:::
